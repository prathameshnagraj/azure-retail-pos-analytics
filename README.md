# azure-retail-pos-analytics

# üêî Conagra Poultry POS Analytics Platform

> End-to-end Azure data engineering solution processing retail Point-of-Sale data with medallion 
> architecture (Raw ‚Üí Bronze ‚Üí Silver/Curated)

[![Azure](https://img.shields.io/badge/Azure-0078D4?style=for-the-badge&logo=microsoftazure&logoColor=white)](https://azure.microsoft.com/)
[![Data Factory](https://img.shields.io/badge/Azure_Data_Factory-0078D4?style=for-the-badge&logo=microsoftazure&logoColor=white)](https://azure.microsoft.com/en-us/products/data-factory/)
[![ADLS Gen2](https://img.shields.io/badge/ADLS_Gen2-0078D4?style=for-the-badge&logo=microsoftazure&logoColor=white)](https://azure.microsoft.com/en-us/products/storage/data-lake-storage/)
[![Parquet](https://img.shields.io/badge/Parquet-50ABF1?style=for-the-badge&logo=apache&logoColor=white)](https://parquet.apache.org/)

![Project Status](https://img.shields.io/badge/Raw_‚Üí_Bronze_‚Üí_Silver-Complete-success?style=flat-square)
![Next Phase](https://img.shields.io/badge/Gold_Layer-Planned-blue?style=flat-square)
![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)

---

## üìã Table of Contents
- [Project Overview](#-project-overview)
- [Business Context](#-business-context)
- [Architecture](#-architecture)
- [What I Built](#-what-i-built)
- [Technical Implementation](#-technical-implementation)
- [Azure Resources](#-azure-resources)
- [Data Pipeline Details](#-data-pipeline-details)
- [Key Features](#-key-features)
- [Challenges & Solutions](#-challenges--solutions)
- [Outcomes & Learnings](#-outcomes--learnings)
- [Repository Structure](#-repository-structure)
- [Setup Guide](#-setup-guide)
- [Future Enhancements](#-future-enhancements)
- [Screenshots](#-screenshots)
- [Contact](#-contact)

---

## üéØ Project Overview

An **enterprise-grade Azure data platform** designed for retail analytics, processing 5 years (2020-2024) of frozen/refrigerated poultry Point-of-Sale data from Excel sources to analytics-ready Parquet datasets.

This project demonstrates a **production-realistic medallion architecture** implementation using Azure-native services:

‚úÖ **Raw Layer** - Immutable Excel file preservation  
‚úÖ **Bronze Layer** - Standardized CSV with schema validation  
‚úÖ **Silver/Curated Layer** - Enriched Parquet ready for BI consumption  
üîÑ **Gold Layer** - Dimensional modeling (planned next phase)  

**Project Timeline:** November 2025 ‚Äì December 2025  
**Data Scope:** 5 years POS data + Product Master dimension  
**Purpose:** Portfolio project showcasing Azure data engineering best practices  

---

## üíº Business Context

### **Objective**
Build an end-to-end data platform for **Conagra-style POS analytics** that:
- Processes multi-year retail transaction data
- Enriches POS transactions with product attributes
- Enables year-over-year trend analysis
- Supports category and subcategory performance tracking
- Provides a foundation for BI dashboards and advanced analytics

### **Business Value**
- **Centralized data repository** for historical POS analysis
- **Automated ingestion** eliminating manual Excel processing
- **Data quality enforcement** through validation and standardization
- **Performance optimization** via Parquet columnar storage
- **Scalable architecture** ready for incremental loads and dimensional modeling

---

## üèóÔ∏è Architecture

### **Medallion Architecture Pattern**

This project implements the industry-standard **medallion (multi-hop) architecture**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          DATA SOURCES                            ‚îÇ
‚îÇ   ‚Ä¢ POS Excel Files (2020-2024)                                 ‚îÇ
‚îÇ   ‚Ä¢ Product Attributes Master Data                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              AZURE DATA FACTORY (Orchestration)                  ‚îÇ
‚îÇ  Pipeline 1: pl_ingest_poultry_data                             ‚îÇ
‚îÇ    ‚Üí Copy Excel ‚Üí CSV (Bronze)                                  ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  Pipeline 2: pl_transform_poultry_curated                       ‚îÇ
‚îÇ    ‚Üí Mapping Data Flow (Bronze ‚Üí Silver)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           ADLS GEN2: stconagralake001 (Data Lake)               ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ üü• RAW LAYER                                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Container: raw/                                           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Original Excel files (immutable)                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Schema inspection proxy files                           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Format: .xlsx                                             ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                          ‚Üì                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ üü® BRONZE LAYER                                           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Container: bronze/                                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Flattened CSV from Excel                                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Schema preserved "as-is"                                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Partitioned by year (year=2020/, year=2021/, ...)       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Format: .csv                                              ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                          ‚Üì                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ üü© SILVER / CURATED LAYER                                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Container: curated/                                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Cleaned & standardized schemas                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ POS ‚Üê LEFT JOIN ‚Üí Product Attributes                    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ UPC normalized, dates parsed, types cast                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Derived business fields                                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Format: .parquet (Snappy compression)                     ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                          ‚Üì                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ üü¶ GOLD LAYER (Planned - Not Implemented)                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Fact_POS_Transactions                                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Dim_Product, Dim_Date, Dim_Store                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Pre-aggregated metrics                                  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   ANALYTICS CONSUMPTION                          ‚îÇ
‚îÇ  ‚Ä¢ Power BI Dashboards (future)                                 ‚îÇ
‚îÇ  ‚Ä¢ Azure Synapse Analytics (future)                             ‚îÇ
‚îÇ  ‚Ä¢ Python/Pandas Analysis                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ‚úÖ What I Built

This project implements **Raw ‚Üí Bronze ‚Üí Silver** layers (Gold deferred to next phase):

### **Layer 1: Raw (Immutable Source Preservation)** ‚úÖ

**Purpose:** Archive original source files for auditability and reprocessing

**Implementation:**
- Container: `raw/uploads/`
- Stored 5 POS Excel files (2020-2024)
- Stored Product Attributes master file
- Created schema proxy file for ADF dataset inspection

**Value:**  
Immutable audit trail; ability to reprocess from source if business logic changes

---

### **Layer 2: Bronze (Standardized Landing Zone)** ‚úÖ

**Purpose:** Convert Excel to CSV with minimal transformation

**Implementation:**
- Container: `bronze/poultry-pos-csv/` and `bronze/product-master/`
- Parameterized ADF pipeline processing years 2020-2024
- ForEach loop enabling parallel year processing
- Year-based folder partitioning: `year=2020/`, `year=2021/`, etc.
- Schema preserved "as-is" from source

**Pipeline:** `pl_ingest_poultry_data`

**Key Activities:**
1. Copy Activity: Product Attributes Excel ‚Üí CSV
2. ForEach Loop (Years 2020-2024):
   - Copy Activity: POS Excel ‚Üí CSV (parameterized by year)

**Value:**  
Queryable CSV format; foundation for transformation layer

---

### **Layer 3: Silver / Curated (Analytics-Ready)** ‚úÖ

**Purpose:** Clean, enrich, and optimize data for analytics consumption

**Implementation:**
- Container: `curated/poultry_pos_enriched/`
- Mapping Data Flow with comprehensive transformations
- POS transactions LEFT JOIN Product Attributes
- Parquet format with Snappy compression
- Fully validated and business-ready dataset

**Pipeline:** `pl_transform_poultry_curated`

**Transformations Applied:**
1. **Schema Standardization** - Column renaming to snake_case
2. **UPC Normalization** - Standardized to 13-digit format (`upc_13`)
3. **Date Parsing** - Handled dirty date formats ("Week Ending 01-07-24")
4. **Type Casting** - String ‚Üí Numeric conversions
5. **Data Quality** - Null handling, range validation
6. **Enrichment** - LEFT JOIN with Product Attributes on `upc_13`
7. **Derived Fields** - Business logic calculations

**Value:**  
Single source of truth for analytics; optimized for query performance

---

## üîß Technical Implementation

### **Data Sources**

**POS Transaction Files:**
- `Fz_Rfg Processed Poultry_POS_2020.xlsx`
- `Fz_Rfg Processed Poultry_POS_2021.xlsx`
- `Fz_Rfg Processed Poultry_POS_2022.xlsx`
- `Fz_Rfg Processed Poultry_POS_2023.xlsx`
- `Fz_Rfg Processed Poultry_POS_2024.xlsx`

**Product Master:**
- `Product Attributes.xlsx`

**Sample/Proxy Files:**
- `pos_sample.xlsx` (for ADF schema inspection)

---

### **Azure Data Factory Objects**

#### **Linked Services**

| Name | Type | Purpose | Authentication |
|------|------|---------|----------------|
| `ls_adls_conagra` | ADLS Gen2 | Data lake connection | Managed Identity |

**Permissions:**  
- Role: `Storage Blob Data Contributor` on `stconagralake001`

---

#### **Datasets**

**Source Datasets (Raw):**
| Name | Format | Parameterized | Purpose |
|------|--------|---------------|---------|
| `ds_excel_pos_raw` | Excel | ‚úÖ Year | POS source files |
| `ds_excel_attributes_raw` | Excel | ‚ùå | Product master |

**Bronze Datasets:**
| Name | Format | Parameterized | Purpose |
|------|--------|---------------|---------|
| `ds_csv_poultry_raw` | CSV | ‚úÖ Year | Bronze POS landing |
| `ds_csv_attributes_bronze` | CSV | ‚ùå | Bronze product master |

**Curated Dataset:**
| Name | Format | Compression | Purpose |
|------|--------|-------------|---------|
| `ds_parquet_pos_curated` | Parquet | Snappy | Final enriched output |

---

#### **Pipelines**

**Pipeline 1: `pl_ingest_poultry_data`** (Bronze Layer)

**Purpose:** Historical one-time ingestion from Excel ‚Üí CSV

**Activities:**
1. **Copy Activity:** Product Attributes
   - Source: `raw/uploads/Product Attributes.xlsx`
   - Sink: `bronze/product-master/`
   
2. **ForEach Activity:** Years [2020, 2021, 2022, 2023, 2024]
   - **Copy Activity (inside loop):** POS by Year
   - Source: `raw/uploads/Fz_Rfg Processed Poultry_POS_{year}.xlsx`
   - Sink: `bronze/poultry-pos-csv/year={year}/`
   - Parameter: `@item()` (current year)

**Pipeline JSON Snippet:**
```json
{
  "name": "ForEachYear",
  "type": "ForEach",
  "typeProperties": {
    "items": {
      "value": "@pipeline().parameters.yearList",
      "type": "Expression"
    },
    "isSequential": false,
    "activities": [
      {
        "name": "CopyPOSByYear",
        "type": "Copy",
        "inputs": [{
          "parameters": {
            "year": "@item()"
          }
        }]
      }
    ]
  }
}
```

---

**Pipeline 2: `pl_transform_poultry_curated`** (Silver Layer)

**Purpose:** Orchestrate Mapping Data Flow for Bronze ‚Üí Curated transformation

**Activities:**
1. **ForEach Activity:** Years [2020, 2021, 2022, 2023, 2024]
   - **Data Flow Activity (inside loop):** `df_refine_poultry_data`
   - Runtime parameter: Current year via `@item()`
   - Processes one year at a time through transformation logic

**Design Rationale:**
- ForEach loop enables year-by-year processing
- Runtime parameters passed to Data Flow dynamically
- Transformation logic centralized in single reusable Data Flow

---

#### **Mapping Data Flow: `df_refine_poultry_data`**

**Purpose:** Core transformation logic (cleansing, enrichment, optimization)

**Data Flow Architecture:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Source: POS (Bronze)   ‚îÇ      ‚îÇ Source: Attributes       ‚îÇ
‚îÇ  bronze/poultry-pos-csv ‚îÇ      ‚îÇ bronze/product-master    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                               ‚îÇ
            ‚Üì                               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Transform: POS         ‚îÇ      ‚îÇ Transform: Attributes    ‚îÇ
‚îÇ  ‚Ä¢ snake_case columns   ‚îÇ      ‚îÇ ‚Ä¢ snake_case columns     ‚îÇ
‚îÇ  ‚Ä¢ UPC ‚Üí upc_13         ‚îÇ      ‚îÇ ‚Ä¢ UPC ‚Üí upc_13           ‚îÇ
‚îÇ  ‚Ä¢ Date parsing         ‚îÇ      ‚îÇ ‚Ä¢ De-duplication         ‚îÇ
‚îÇ  ‚Ä¢ Type casting         ‚îÇ      ‚îÇ                          ‚îÇ
‚îÇ  ‚Ä¢ Derived fields       ‚îÇ      ‚îÇ                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                               ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ   Join (LEFT OUTER)  ‚îÇ
                  ‚îÇ   Key: upc_13        ‚îÇ
                  ‚îÇ   POS ‚Üê Attributes   ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚Üì
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ   Sink: Parquet      ‚îÇ
                  ‚îÇ   curated/           ‚îÇ
                  ‚îÇ   poultry_pos_       ‚îÇ
                  ‚îÇ   enriched/          ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Transformations:**

**1. Column Standardization**
```
// Rename columns to snake_case for consistency
original_column_name ‚Üí standardized_column_name
```

**2. UPC Normalization**
```
// Standardize all UPCs to 13 digits
upc_13 = 
  iif(
    length(toString(raw_upc)) == 12,
    concat('0', toString(raw_upc)),
    toString(raw_upc)
  )
```

**3. Date Parsing (Dirty Data Handling)**
```
// Handle format: "Week Ending 01-07-24"
// Extract and parse embedded date
parsed_date = toDate(
  substring(week_ending_column, 13, 8),
  'MM-dd-yy'
)
```

**4. Type Casting**
```
// Convert string columns to appropriate numeric types
amount_numeric = toDecimal(amount_string)
quantity_int = toInteger(quantity_string)
```

**5. Data Quality & Safety**
```
// Handle nulls without dropping records
coalesce(column_value, default_value)

// Validate ranges
iif(amount > 0 AND amount < 10000, amount, null())
```

**6. Product Enrichment (JOIN)**
```sql
-- LEFT OUTER JOIN to preserve all POS records
SELECT 
  pos.*,
  attr.product_name,
  attr.category,
  attr.subcategory,
  attr.brand,
  attr.manufacturer
FROM POS_Bronze pos
LEFT OUTER JOIN Product_Attributes attr
  ON pos.upc_13 = attr.upc_13
```

**7. Sink Configuration**
- **Format:** Parquet
- **Compression:** Snappy
- **Write Mode:** Overwrite (full refresh)
- **Target:** `curated/poultry_pos_enriched/`

---

## üíé Key Features

### **1. Enterprise Medallion Architecture**
- **Industry-standard** multi-hop pattern (Raw ‚Üí Bronze ‚Üí Silver ‚Üí Gold)
- **Clear separation** of concerns across layers
- **Reprocessing capability** through immutable raw layer
- **Audit trail** with full data lineage

### **2. Parameterized Pipeline Design**
- **ForEach loops** processing multiple years in parallel
- **Dynamic parameters** eliminating code duplication
- **Runtime flexibility** through pipeline expressions
- **Scalable framework** for adding new data sources

### **3. Production-Grade Data Transformations**
- **Schema standardization** (snake_case conventions)
- **UPC normalization** (12-digit ‚Üí 13-digit)
- **Dirty date parsing** (complex string extraction)
- **Type safety** (explicit casting with null handling)
- **Data quality checks** (validation without data loss)

### **4. Managed Identity Authentication**
- **Credential-free** Azure resource access
- **RBAC-based** permissions (Storage Blob Data Contributor)
- **Security best practice** (no connection strings in code)

### **5. Parquet Optimization**
- **Columnar storage** for analytics performance
- **Snappy compression** reducing storage costs
- **Schema evolution** support for future changes

### **6. Reusable Data Flow**
- **Centralized transformation logic** in single Data Flow
- **Year-agnostic processing** through parameters
- **Visual ETL design** for maintainability

---

## üöß Challenges & Solutions

### **Challenge 1: Dirty Date Format Parsing**

**Problem:**  
Source Excel files contained dates in non-standard format:
```
"Week Ending 01-07-24"
```
Standard date parsing functions couldn't handle this string format.

**Solution:**  
Implemented substring extraction + date parsing in Mapping Data Flow:
```
// Extract "01-07-24" from "Week Ending 01-07-24"
date_string = substring(week_ending_column, 13, 8)

// Parse extracted string
parsed_date = toDate(date_string, 'MM-dd-yy')
```

**Outcome:**  
100% of dates successfully parsed; no data loss

---

### **Challenge 2: UPC Code Standardization**

**Problem:**  
- POS files contained mix of 12-digit and 13-digit UPC codes
- Product Attributes used 13-digit standard exclusively
- Direct join resulted in ~30% match failures

**Solution:**  
Standardized all UPCs to 13 digits by left-padding with zero:
```
upc_13 = iif(
  length(toString(upc)) == 12,
  concat('0', toString(upc)),
  toString(upc)
)
```

**Outcome:**  
Join match rate improved from 70% ‚Üí 98%+

---

### **Challenge 3: Year-Based Physical Partitioning Complexity**

**Problem:**  
Attempted to implement physical Parquet partitioning by year (`year=2020/`, `year=2021/`, etc.) in Curated layer, but ADF Mapping Data Flow partition configuration became overly complex for portfolio project scope.

**Solution:**  
- **Accepted trade-off:** No physical year partitions in Curated layer
- **Mitigation:** Date column enables logical partitioning for queries
- **Justification:** Data volume (5 years) doesn't warrant partition overhead yet

**Outcome:**  
- Cleaner ADF pipeline design
- Data correctness and usability unaffected
- Partitioning can be added in Gold layer when needed

**Design Decision Rationale:**  
This is an **acceptable, defensible trade-off** for a portfolio project. Physical partitioning delivers value at TB scale; current dataset is MB/GB range where benefits are minimal.

---

### **Challenge 4: Excel Source Schema Inspection**

**Problem:**  
ADF datasets require schema definition, but Excel files have dynamic schemas that vary by year.

**Solution:**  
- Created `pos_sample.xlsx` as schema proxy file
- Used proxy for ADF dataset configuration
- Enabled schema drift in Mapping Data Flow to handle variations

**Outcome:**  
Pipeline resilient to minor schema changes across years

---

## üìä Outcomes & Learnings

### **‚úÖ Technical Achievements**

**Data Engineering:**
- Successfully processed **5 years** of retail POS data (2020-2024)
- Implemented **enterprise medallion architecture** (Raw ‚Üí Bronze ‚Üí Silver)
- Built **reusable transformation framework** via Mapping Data Flows
- Achieved **98%+ join match rate** after UPC standardization

**Azure Platform:**
- Configured **ADLS Gen2** with hierarchical namespace
- Implemented **Managed Identity** authentication (zero credentials in code)
- Created **parameterized ADF pipelines** eliminating duplication
- Designed **year-partitioned** Bronze layer for organized data access

**Data Quality:**
- Handled **dirty date formats** (complex string parsing)
- Standardized **UPC codes** across format variations
- Implemented **null-safe transformations** (no data loss)
- Applied **type casting** with validation

**Performance & Optimization:**
- Leveraged **Parquet columnar format** for analytics workloads
- Applied **Snappy compression** reducing storage costs
- Enabled **parallel processing** through ForEach loops

---

### **üìö Key Learnings**

**1. Medallion Architecture is Essential for Enterprise Data**
- **Raw layer** provides audit trail and reprocessing capability
- **Bronze layer** standardizes format while preserving source schema
- **Silver layer** applies business logic without impacting upstream
- **Gold layer** (future) will serve dimensional modeling needs

**2. Managed Identity > Connection Strings**
- Eliminates credential management overhead
- Follows Azure security best practices
- Simplifies deployment across environments (Dev/Test/Prod)

**3. Parameterization Prevents Pipeline Sprawl**
- One parameterized pipeline >> Five hard-coded pipelines
- ForEach loops enable scalable year/source processing
- Future data sources onboarded by changing parameters, not code

**4. Parquet is Superior for Analytics**
- **Compression:** ~70% storage reduction vs CSV
- **Performance:** Columnar format optimized for aggregations
- **Schema evolution:** Built-in support for adding columns
- **Trade-off:** Not human-readable (keep Raw as Excel for inspection)

**5. Real-World Data is Messy**
- Standard date parsers fail on "Week Ending 01-07-24" format
- UPC codes come in multiple digit lengths
- Type inference can fail on numeric-looking strings
- **Lesson:** Always inspect source data before designing transformations

**6. Design Trade-Offs are Acceptable (When Documented)**
- Skipping physical partitioning in Curated layer = **valid decision**
- Documenting "why we didn't do X" shows maturity
- Portfolio projects should prioritize **data correctness > micro-optimizations**

**7. ADF Mapping Data Flows vs Databricks**
- **For SQL-like ETL** (joins, filters, type casting): ADF Data Flows sufficient
- **For complex analytics** (ML, advanced window functions): Databricks better
- My use case fit ADF perfectly; avoided over-engineering

---

### **üéØ Skills Demonstrated**

‚úÖ **Azure Data Factory**
- Copy activities with parameterized datasets
- Mapping Data Flows (visual ETL design)
- ForEach loops and pipeline orchestration
- Linked Services and Managed Identity auth
- Dataset schema configuration

‚úÖ **Azure Data Lake Storage Gen2**
- Hierarchical namespace design
- Folder organization (medallion layers)
- RBAC permissions (Storage Blob Data Contributor)
- Container and directory structure planning

‚úÖ **Data Engineering Patterns**
- Medallion architecture implementation
- Parameterized pipeline framework
- Schema standardization techniques
- Data quality validation patterns

‚úÖ **Parquet & Performance Optimization**
- Columnar storage benefits
- Snappy compression
- Trade-off analysis (partitioning decisions)

‚úÖ **Problem Solving**
- Parsing non-standard date formats
- UPC code normalization
- Excel schema handling in ADF
- Design trade-off documentation

---

## üöÄ Setup & Deployment

### **Prerequisites**

- Azure subscription with Owner or Contributor access
- Azure CLI installed ([Install Guide](https://docs.microsoft.com/cli/azure/install-azure-cli))
- PowerShell 7+ (for automation scripts)
- Basic understanding of Azure Data Factory and ADLS Gen2

---

### **Step 1: Create Azure Resources**

**1.1 Resource Group**
```bash
az group create \
  --name rg-conagra-poultry-analytics \
  --location eastus
```

**1.2 Storage Account (ADLS Gen2)**
```bash
az storage account create \
  --name stconagralake001 \
  --resource-group rg-conagra-poultry-analytics \
  --location eastus \
  --sku Standard_LRS \
  --kind StorageV2 \
  --hierarchical-namespace true
```

**1.3 Create Containers**
```bash
# Raw container
az storage container create \
  --name raw \
  --account-name stconagralake001

# Bronze container
az storage container create \
  --name bronze \
  --account-name stconagralake001

# Curated container
az storage container create \
  --name curated \
  --account-name stconagralake001
```

---

### **Step 2: Create Azure Data Factory**

```bash
az datafactory create \
  --name adf-conagra-poultry-ingest \
  --resource-group rg-conagra-poultry-analytics \
  --location eastus
```

---

### **Step 3: Configure Managed Identity**

**3.1 Enable System-Assigned Managed Identity** (automatically created with ADF)

**3.2 Grant Storage Permissions**
```bash
# Get ADF Managed Identity Object ID
ADF_PRINCIPAL_ID=$(az datafactory show \
  --name adf-conagra-poultry-ingest \
  --resource-group rg-conagra-poultry-analytics \
  --query identity.principalId \
  --output tsv)

# Get Storage Account Resource ID
STORAGE_ID=$(az storage account show \
  --name stconagralake001 \
  --resource-group rg-conagra-poultry-analytics \
  --query id \
  --output tsv)

# Assign Storage Blob Data Contributor role
az role assignment create \
  --assignee $ADF_PRINCIPAL_ID \
  --role "Storage Blob Data Contributor" \
  --scope $STORAGE_ID
```

---

### **Step 4: Import ADF Artifacts**

**4.1 Clone Repository**
```bash
git clone https://github.com/prathameshnagraj/azure-conagra-poultry-analytics.git
cd azure-conagra-poultry-analytics
```

**4.2 Import Linked Services**
1. Open Azure Data Factory Studio
2. Go to **Manage** ‚Üí **Linked Services**
3. Click **+ New**
4. Import `02-adf-pipelines/linked-services/ls_adls_conagra.json`
5. Update storage account name to your `stconagralake001`

**4.3 Import Datasets**
1. Go to **Author** ‚Üí **Datasets**
2. Import all JSON files from `02-adf-pipelines/datasets/`
3. Verify linked service references

**4.4 Import Pipelines**
1. Go to **Author** ‚Üí **Pipelines**
2. Import `pl_ingest_poultry_data.json`
3. Import `pl_transform_poultry_curated.json`

**4.5 Import Mapping Data Flows**
1. Go to **Author** ‚Üí **Data Flows**
2. Import `df_refine_poultry_data.json`

---

### **Step 5: Upload Sample Data**

**5.1 Upload to Raw Layer**
```bash
az storage blob upload \
  --account-name stconagralake001 \
  --container-name raw \
  --name uploads/pos_sample_2024.xlsx \
  --file 04-data-samples/raw-sample/pos_sample_2024.xlsx
```

---

### **Step 6: Test Pipeline Execution**

**6.1 Test Ingestion Pipeline**
1. Open ADF Studio ‚Üí **Author** ‚Üí **Pipelines** ‚Üí `pl_ingest_poultry_data`
2. Click **Debug** (use sample data)
3. Monitor execution in **Monitor** tab
4. Verify output in `bronze/` container

**6.2 Test Transformation Pipeline**
1. Open `pl_transform_poultry_curated`
2. Click **Debug**
3. Monitor Data Flow execution
4. Verify Parquet output in `curated/poultry_pos_enriched/`

---

### **üìò Detailed Setup Guide**

For complete deployment instructions, see:
- [05-documentation/setup-guide.md](05-documentation/setup-guide.md)
- [06-deployment/prerequisites.md](06-deployment/prerequisites.md)

---

## üöÄ Future Enhancements (Roadmap)

This project currently implements **Raw ‚Üí Bronze ‚Üí Silver** layers. The following enhancements are planned:

### **Phase 4: Gold Layer (Dimensional Modeling)** üîÑ

**Objective:** Build star schema for optimized BI consumption

**Planned Implementation:**
- **Fact Table:** `Fact_POS_Transactions`
  - Grain: One row per UPC per store per date
  - Measures: Sales amount, quantity, units
  - Foreign keys to dimensions

- **Dimension Tables:**
  - `Dim_Product` (product, category, subcategory, brand)
  - `Dim_Date` (date, week, month, quarter, year)
  - `Dim_Store` (store ID, location, region)

- **Aggregations:**
  - Pre-aggregated daily/weekly/monthly metrics
  - Year-over-year comparison tables

**Expected Value:**
- 10x faster dashboard query performance
- Simplified BI model for end users
- Historical trend analysis enabled

---

### **Phase 5: Incremental Load Patterns** üîÑ

**Objective:** Move from full refresh to incremental processing

**Planned Implementation:**
- **Watermark Table:** Track last processed date/file
- **Change Data Capture:** Process only new/changed records
- **Upsert Logic:** MERGE operations in Gold layer
- **Idempotent Pipelines:** Safe to rerun without duplicates

**Expected Value:**
- 90% reduction in daily processing time
- Lower data transfer costs
- Near real-time data availability

---

### **Phase 6: Power BI Dashboard** üîÑ

**Objective:** Visualize curated data for business insights

**Planned Dashboards:**
- Year-over-year sales trends
- Category/subcategory performance
- Top products by sales volume
- Regional sales comparisons

**Data Source:** Direct Query to Gold layer Parquet files

---

### **Phase 7: CI/CD & DevOps** üîÑ

**Objective:** Automate deployment across environments

**Planned Implementation:**
- Azure DevOps pipelines for ADF deployment
- ARM templates for infrastructure as code
- Environment-specific parameters (Dev/Test/Prod)
- Automated testing framework

---

### **Phase 8: Data Quality Monitoring** üîÑ

**Objective:** Automated data quality tracking and alerting

**Planned Implementation:**
- Row count reconciliation (source vs curated)
- Schema drift detection
- Data quality scorecards
- Email alerts for quality threshold violations

---

**Why Document Future Phases?**  
Shows production awareness, architectural planning, and continuous learning mindset. Demonstrates understanding of enterprise data platform evolution.

---

## üì∏ Screenshots

### **Azure Data Factory: Ingestion Pipeline**
![Ingestion Pipeline](07-screenshots/adf-pipeline-success.png)
*ForEach loop processing years 2020-2024 in parallel*

---

### **Azure Data Factory: Transformation Pipeline**
![Transformation Pipeline](07-screenshots/foreach-loop-config.png)
*Pipeline orchestrating Mapping Data Flow with year parameters*

---

### **Mapping Data Flow Canvas**
![Data Flow Canvas](07-screenshots/mapping-dataflow-canvas.png)
*Visual transformation logic: UPC normalization, date parsing, product join*

---

### **ADLS Gen2: Bronze Layer Structure**
![ADLS Structure](07-screenshots/adls-folder-structure.png)
*Year-partitioned folder organization (year=2020/, year=2021/, etc.)*

---

### **Curated Layer: Parquet Output**
![Parquet Output](07-screenshots/curated-parquet-output.png)
*Final analytics-ready dataset in compressed Parquet format*

---

## üõ†Ô∏è Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Orchestration** | Azure Data Factory | Pipeline scheduling, workflow management |
| **Storage** | ADLS Gen2 (stconagralake001) | Hierarchical data lake with POSIX compliance |
| **File Formats** | Excel (Raw), CSV (Bronze), Parquet (Silver) | Layer-appropriate formats |
| **Transformations** | ADF Mapping Data Flows | Visual ETL with Spark backend |
| **Authentication** | Managed Identity | Secure, credential-free Azure access |
| **Compression** | Snappy (Parquet) | Fast compression for analytics workloads |
| **Architecture** | Medallion (Raw‚ÜíBronze‚ÜíSilver‚ÜíGold) | Industry-standard multi-hop pattern |

**Azure Services Used:**
- Azure Resource Group
- Azure Data Lake Storage Gen2
- Azure Data Factory
- Azure Managed Identity / RBAC

---

## üí° Design Decisions & Trade-Offs

### **Decision 1: No Physical Year Partitions in Curated Layer**

**Context:** Attempted Parquet partitioning by year (`year=2020/`, etc.) in Silver layer

**Trade-Off:**
- ‚ùå **Not Implemented:** Physical folder partitions in Curated
- ‚úÖ **Implemented:** Date column for logical partitioning

**Rationale:**
- ADF Data Flow partition configuration added significant complexity
- Current data volume (5 years, MB/GB range) doesn't warrant partition overhead
- Date column provides sufficient filtering for queries
- Physical partitioning can be added in Gold layer if needed

**Impact:** Minimal - data correctness and query performance unaffected

**Conclusion:** **Acceptable, defensible decision** for portfolio project scope

---

### **Decision 2: Full Refresh vs Incremental Loads**

**Context:** Current pipelines perform full refresh (overwrite mode)

**Trade-Off:**
- ‚ùå **Not Implemented:** Incremental/CDC patterns
- ‚úÖ **Implemented:** Full historical ingestion framework

**Rationale:**
- Source data is file-based (not streaming)
- Historical one-time load appropriate for portfolio demonstration
- Incremental patterns designed conceptually (Phase 5 roadmap)

**Impact:** Pipeline runs reprocess all data; acceptable for demo/portfolio

**Conclusion:** Foundation in place for incremental evolution

---

### **Decision 3: ADF Mapping Data Flows vs Databricks**

**Context:** Choice of transformation engine

**Trade-Off:**
- ‚úÖ **Selected:** ADF Mapping Data Flows
- ‚ùå **Not Used:** Azure Databricks

**Rationale:**
- Transformations are SQL-like (joins, filters, type casting)
- No complex analytics or ML required
- Lower operational overhead (no cluster management)
- Cost: ADF $0.25/hour vs Databricks $5+/hour

**Impact:** Sufficient for use case; avoided over-engineering

**Conclusion:** Right tool for the job

---

## üì¨ Contact

**Prathamesh Nagraj**  
üìß Email: [ppnagraj.work@gmail.com](mailto:ppnagraj.work@gmail.com)  
üíº LinkedIn: [linkedin.com/in/prathamesh-nagraj](https://www.linkedin.com/in/prathamesh-nagraj/)  
üêô GitHub: [@prathameshnagraj](https://github.com/prathameshnagraj)  
üìÑ Resume: [View PDF](https://drive.google.com/file/d/1m2xUO6h7FuuSj6vhiz0Nj_iqSkUlKFIo/view)

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **Architecture Pattern:** Medallion architecture inspired by Databricks best practices
- **Data Source:** Conagra-style poultry POS data (synthetic for portfolio)
- **Azure Documentation:** Microsoft Learn and Azure Architecture Center

---

<p align="center">
  <i>‚≠ê If you find this project helpful, please consider starring the repository!</i>
</p>

<p align="center">
  <sub>Built with Azure Data Factory, ADLS Gen2, and Parquet | December 2025</sub>
</p>

<p align="center">
  <sub>Demonstrating production-realistic data engineering patterns for portfolio and interview preparation</sub>
</p>
